{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lupam\\OneDrive\\Escritorio\\GitHub\\BamTech_choroidalyzer\\src\n",
      "c:\\Users\\lupam\\OneDrive\\Escritorio\\GitHub\\BamTech_choroidalyzer\\src\\lib\n",
      "c:\\Users\\lupam\\OneDrive\\Escritorio\\GitHub\\BamTech_choroidalyzer\\src\\utility\n",
      "c:\\Users\\lupam\\OneDrive\\Escritorio\\GitHub\\BamTech_choroidalyzer\n",
      "c:\\Users\\lupam\\OneDrive\\Escritorio\\GitHub\\BamTech_choroidalyzer\\data\n",
      "c:\\Users\\lupam\\OneDrive\\Escritorio\\GitHub\\BamTech_choroidalyzer\\src\\models\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#Import access to other files\n",
    "sys.path.append(\"..\")  # Agrega la carpeta padre de `src` al path de búsqueda\n",
    "import utility.access_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fun_copy_images import copy_images\n",
    "\n",
    "#Copio images from raw to preprocessed\n",
    "folder_raw = \"../../pilar_data/OC009_2023_08_15/raw\"\n",
    "folder_preprocessed = \"../../pilar_data/OC009_2023_08_15/preprocessed\"\n",
    "\n",
    "copy_images(folder_raw, folder_preprocessed)\n",
    "\n",
    "#Defino files como los nombres de todos los files dentro de folder\n",
    "files = os.listdir(folder_preprocessed)\n",
    "#Añado a cada file la dirección de la carpeta\n",
    "files = [folder_preprocessed + '/' + file for file in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop\n",
    "from fun_crop import crop\n",
    "coordenadas = (504, 0, 1008, 504)\n",
    "\n",
    "#Corroboro que las coordenadas correspondan a imagen cuadrada\n",
    "if coordenadas[2] - coordenadas[0] != coordenadas[3] - coordenadas[1]:\n",
    "    raise ValueError(\"Las coordenadas no corresponden a una imagen cuadrada\")\n",
    "    \n",
    "files = crop(files, coordenadas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaleo a 768x768\n",
    "from fun_scale import scale\n",
    "\n",
    "files = scale(files, (768, 768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gray-scale\n",
    "from fun_gray_scale import gray_scale\n",
    "\n",
    "files = gray_scale(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copio images from preprocessed to processed\n",
    "folder_processed = \"../../pilar_data/OC009_2023_08_15/processed\"\n",
    "copy_images(folder_preprocessed, folder_processed)\n",
    "\n",
    "files = os.listdir(folder_processed)\n",
    "files = [folder_processed + '/' + file for file in files]\n",
    "\n",
    "#Por alguna razón que desconozco, hay que volver a pasar a la escala de grises\n",
    "files = gray_scale(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Offset 15 too far to the right, choosing index 756\n",
      "WARNING:root:Offset 15 too far to the right, choosing index 756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0]), 0, 0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(file, img)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# basic useage: get the metrics\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mchoroidalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, scale=(11.49, 3.87)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# choroidalyzer also has a basic plotting function to inspect segmentation outputs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\BamTech_choroidalyzer3\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lupam\\OneDrive\\Escritorio\\GitHub\\BamTech_choroidalyzer\\src\\models\\choroidalyze\\inference.py:94\u001b[0m, in \u001b[0;36mChoroidalyzer.analyze\u001b[1;34m(self, img_path_or_object, thresholds, scale)\u001b[0m\n\u001b[0;32m     88\u001b[0m k \u001b[38;5;241m=\u001b[39m compute_measurement(reg_mask\u001b[38;5;241m=\u001b[39mregion_mask,\n\u001b[0;32m     89\u001b[0m                                                                                vess_mask\u001b[38;5;241m=\u001b[39mvessel_mask,\n\u001b[0;32m     90\u001b[0m                                                                                fovea\u001b[38;5;241m=\u001b[39mfov_loc,\n\u001b[0;32m     91\u001b[0m                                                                                scale\u001b[38;5;241m=\u001b[39mscale)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(k)\n\u001b[1;32m---> 94\u001b[0m raw_thickness, area, vascular_index, choroid_vessel_area \u001b[38;5;241m=\u001b[39m compute_measurement(reg_mask\u001b[38;5;241m=\u001b[39mregion_mask,\n\u001b[0;32m     95\u001b[0m                                                                                vess_mask\u001b[38;5;241m=\u001b[39mvessel_mask,\n\u001b[0;32m     96\u001b[0m                                                                                fovea\u001b[38;5;241m=\u001b[39mfov_loc,\n\u001b[0;32m     97\u001b[0m                                                                                scale\u001b[38;5;241m=\u001b[39mscale)\n\u001b[0;32m     98\u001b[0m thickness \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(raw_thickness)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthickness\u001b[39m\u001b[38;5;124m'\u001b[39m: thickness, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m: area, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvascular_index\u001b[39m\u001b[38;5;124m'\u001b[39m: vascular_index,\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvessel_area\u001b[39m\u001b[38;5;124m'\u001b[39m: choroid_vessel_area, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_thickness\u001b[39m\u001b[38;5;124m'\u001b[39m: raw_thickness}\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "\n",
    "from choroidalyze import Choroidalyzer\n",
    "\n",
    "# This initialises choroidalyzer. \n",
    "# It will try to automatically download the model weights from github the first time you run it. \n",
    "choroidalyzer = Choroidalyzer()\n",
    "\n",
    "\n",
    "\n",
    "#Cargo imagen de ejemplo\n",
    "file = files[1]\n",
    "#Convierto imagen a blanco y negro\n",
    "img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imwrite(file, img)\n",
    "\n",
    "# basic useage: get the metrics\n",
    "metrics = choroidalyzer.analyze(file) #, scale=(11.49, 3.87)\n",
    "print(metrics)\n",
    "\n",
    "# choroidalyzer also has a basic plotting function to inspect segmentation outputs\n",
    "choroidalyzer.predict_and_plot(file)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fun_analysis_choroidalyzer import analysis_choroidalyzer\n",
    "# from choroidalyze import Choroidalyzer\n",
    "\n",
    "# # This initialises choroidalyzer. \n",
    "# # It will try to automatically download the model weights from github the first time you run it. \n",
    "# choroidalyzer = Choroidalyzer()\n",
    "\n",
    "# for file in files:\n",
    "#     file = gray_scale(file)\n",
    "#     files, metrics = analysis_choroidalyzer(choroidalyzer, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BamTech_choroidalyzer3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
